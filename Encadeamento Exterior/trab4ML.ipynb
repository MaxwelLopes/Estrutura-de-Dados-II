{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  @page {\n",
    "    size\n",
    "    width: 21cm;\n",
    "    height: 29.7cm;\n",
    "    margin: 0 auto; /* Centraliza o conteúdo na página */\n",
    "    padding: 2cm; /* Define margens da página, se desejar */\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CLASSIFICAÇÃO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz confusão\n",
    "\n",
    "A **matriz de confusão** é uma tabela usada para avaliar o desempenho de um modelo de classificação. Ela é composta por quatro valores principais:\n",
    "\n",
    "- **TP (True Positive):** O número de casos em que o modelo previu corretamente a classe positiva (1) e os valores reais também eram positivos (1).\n",
    "- **TN (True Negative):** O número de casos em que o modelo previu corretamente a classe negativa (0) e os valores reais também eram negativos (0).\n",
    "- **FP (False Positive):** O número de casos em que o modelo previu incorretamente a classe positiva (1) quando os valores reais eram negativos (0).\n",
    "- **FN (False Negative):** O número de casos em que o modelo previu incorretamente a classe negativa (0) quando os valores reais eram positivos (1).\n",
    "\n",
    "A matriz de confusão é representada da seguinte forma:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\text{Verdadeiros Positivos (TP)} & \\text{Falsos Positivos (FN)} \\\\\n",
    "\\text{Falsos Negativos (FP)} & \\text{Verdadeiros Negativos (TN)}\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Essa matriz fornece uma representação visual das previsões corretas e incorretas feitas pelo modelo em relação às classes positivas e negativas, permitindo uma avaliação rápida da quantidade de acertos e erros.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_confusao(y_true, y_pred):\n",
    "    # Verifica se as listas têm o mesmo comprimento\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"As listas y_true e y_pred devem ter o mesmo comprimento.\")\n",
    "\n",
    "    # Transformando as listas em arrays NumPy para facilitar os cálculos\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calcula a matriz de confusão\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    # Retorna a matriz de confusão como uma matriz 2x2\n",
    "    matriz_confusao = np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "    return matriz_confusao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 negativo verdadeiro, 1 falso positivo e 7 falsos negativos:\n",
      "\n",
      "[[0 1]\n",
      " [7 1]]\n",
      "\n",
      "\n",
      "2 falsos negativos e 7 positivos verdadeiros\n",
      "\n",
      "[[7 0]\n",
      " [2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"1 negativo verdadeiro, 1 falso positivo e 7 falsos negativos:\\n\")\n",
    "y_true = [0,1,1,1,1,1,1,1,0]\n",
    "y_pred = [0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "print(matriz_confusao(y_true,y_pred))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"2 falsos negativos e 7 positivos verdadeiros\\n\")\n",
    "y_true = [1,1,1,1,1,1,1,1,1]\n",
    "y_pred = [0,1,1,1,1,1,1,1,0]\n",
    "print(matriz_confusao(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acurácia\n",
    "A **acurácia** é uma métrica usada na avaliação de modelos de classificação. Ela mede a proporção de previsões corretas em relação ao total de previsões feitas por um modelo.\n",
    "\n",
    "A fórmula da acurácia é dada por:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Acuracia} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A acurácia representa a porcentagem de previsões corretas feitas por um modelo em relação ao total de previsões realizadas. A acurácia quantifica a precisão geral do modelo e é expressa como uma porcentagem, indicando o quão bem o modelo acertou as previsões em relação ao conjunto de dados de teste. Quanto maior a acurácia, maior a proporção de acertos do modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acuracia(y_true, y_pred):\n",
    "    # Verifica se as listas têm o mesmo comprimento\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"As listas y_true e y_pred devem ter o mesmo comprimento.\")\n",
    "\n",
    "    # Transformando as listas em arrays NumPy para facilitar os cálculos\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Verifica quantas previsões coincidem com os valores reais\n",
    "    corretas = np.sum(y_true == y_pred)\n",
    "\n",
    "    # Calcula a acurácia como a proporção de previsões corretas\n",
    "    acuracia = corretas / len(y_true)\n",
    "\n",
    "    return round(acuracia * 100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 negativo verdadeiro, 1 falso positivo e 7 falsos negativos:\n",
      "\n",
      "11.11\n",
      "11.11% de acurácia\n",
      "\n",
      "\n",
      "2 falsos negativos e 7 positivos verdadeiros\n",
      "\n",
      "77.78\n",
      "77.78% de acurácia\n"
     ]
    }
   ],
   "source": [
    "print(\"1 negativo verdadeiro, 1 falso positivo e 7 falsos negativos:\\n\")\n",
    "y_true = [0,1,1,1,1,1,1,1,0]\n",
    "y_pred = [0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "print(acuracia(y_true,y_pred))\n",
    "print(acuracia(y_true,y_pred),\"% de acurácia\", sep='')\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"2 falsos negativos e 7 positivos verdadeiros\\n\")\n",
    "y_true = [1,1,1,1,1,1,1,1,1]\n",
    "y_pred = [0,1,1,1,1,1,1,1,0]\n",
    "\n",
    "print(acuracia(y_true,y_pred))\n",
    "print(acuracia(y_true,y_pred),\"% de acurácia\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "**Recall** mede a proporção de exemplos positivos reais que foram corretamente identificados pelo modelo. Ele é calculado por:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    # Verifica se as listas têm o mesmo comprimento\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"As listas y_true e y_pred devem ter o mesmo comprimento.\")\n",
    "\n",
    "    # Transformando as listas em arrays NumPy para facilitar os cálculos\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    recall = TP/(TP+FN)\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 falsos negativos e 7 positivos verdadeiros\n",
      "\n",
      "Recall = 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "print(\"2 falsos negativos e 7 positivos verdadeiros\\n\")\n",
    "y_true = [1,1,1,1,1,1,1,1,1]\n",
    "y_pred = [0,1,1,1,1,1,1,1,0]\n",
    "\n",
    "print(\"Recall =\",recall(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "A precisão é uma métrica de avaliação em problemas de classificação que mede a proporção de previsões positivas corretas feitas pelo modelo em relação ao total de previsões positivas feitas.\n",
    "\n",
    "A fórmula para calcular a precisão:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "\\text{Precisão} = \\frac{\\text{TP}}{\\text{TP + FP}}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A precisão é uma métrica importante em cenários em que é crítico evitar falsos positivos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    # Verifica se as listas têm o mesmo comprimento\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"As listas y_true e y_pred devem ter o mesmo comprimento.\")\n",
    "\n",
    "    # Transformando as listas em arrays NumPy para facilitar os cálculos\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 falsos negativos e 7 positivos verdadeiros\n",
      "\n",
      "Precisão = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"2 falsos negativos e 7 positivos verdadeiros\\n\")\n",
    "y_true = [1,1,1,1,1,1,1,1,1]\n",
    "y_pred = [0,1,1,1,1,1,1,1,0]\n",
    "\n",
    "print(\"Precisão =\",precision(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1\n",
    "\n",
    "O F1 é calculado fazendo uma média harmônica entre a precisão (precision) e o recall (revocação), equilibrando assim a capacidade do modelo de fazer previsões corretas e sua capacidade de recuperar exemplos positivos.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{Precisao} \\cdot \\text{Recall}}{\\text{Precisao} + \\text{Recall}}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "O F1 varia de 0 a 1, onde um valor mais alto indica um melhor desempenho do modelo em equilibrar precisão e recall. O F1 é particularmente útil em situações em que as classes estão desequilibradas e você deseja avaliar o modelo de forma abrangente.\n",
    "\n",
    "No entanto, o F1 não leva em consideração a classificação correta de exemplos negativos (TN - Verdadeiros Negativos) e pode não ser a métrica mais adequada em todos os cenários.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(y_true, y_pred):\n",
    "    R = recall(y_true, y_pred)\n",
    "    P = precision(y_true, y_pred)\n",
    "\n",
    "    F1 = 2 * ((P * R)/(P + R))\n",
    "\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 falsos negativos e 7 positivos verdadeiros\n",
      "\n",
      "F1 = 0.8750000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"2 falsos negativos e 7 positivos verdadeiros\\n\")\n",
    "y_true = [1,1,1,1,1,1,1,1,1]\n",
    "y_pred = [0,1,1,1,1,1,1,1,0]\n",
    "\n",
    "print(\"F1 =\", F1(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "O Matthews Correlation Coefficient (MCC) é uma métrica de avaliação de desempenho comumente utilizada em problemas de classificação binária. Ele leva em consideração os quatro elementos da matriz de confusão (verdadeiros positivos, verdadeiros negativos, falsos positivos e falsos negativos) para fornecer uma medida global da qualidade das previsões de um modelo.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "MCC = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "O MCC varia de **-1** a **+1**:\n",
    "\n",
    "- Um MCC de +1 indica uma correspondência perfeita entre as previsões do modelo e os valores reais, onde o modelo faz todas as previsões corretamente.\n",
    "- Um MCC de 0 indica que o modelo está fazendo previsões equivalentes às de um modelo aleatório.\n",
    "- Um MCC de -1 indica um desempenho inversamente perfeito, onde o modelo faz todas as previsões erradas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matthews_correlation_coefficient(y_true, y_pred):\n",
    "    # Verifica se as listas têm o mesmo comprimento\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"As listas y_true e y_pred devem ter o mesmo comprimento.\")\n",
    "\n",
    "    # Transformando as listas em arrays NumPy para facilitar os cálculos\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calcula a matriz de confusão\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    print(TP,FN)\n",
    "    print(FP,TN)\n",
    "\n",
    "\n",
    "    numerador = (TP * TN) - (FP * FN)\n",
    "    denominador = ((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) ** 0.5\n",
    "\n",
    "    if denominador == 0:\n",
    "        return 0.0  # Evitar divisão por zero\n",
    "\n",
    "    mcc = numerador / denominador\n",
    "    return mcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 negativo verdadeiro, 1 falso positivo e 7 falsos negativos:\n",
      "\n",
      "0 7\n",
      "1 1\n",
      "MCC = -0.6614378277661476\n",
      "\n",
      "\n",
      "2 falsos negativos e 7 positivos verdadeiros\n",
      "\n",
      "7 2\n",
      "0 0\n",
      "MCC = 0.0\n",
      "\n",
      "\n",
      "7 positivos verdadeiros e 2 negativos verdadeiros\n",
      "\n",
      "7 0\n",
      "0 2\n",
      "MCC = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"1 negativo verdadeiro, 1 falso positivo e 7 falsos negativos:\\n\")\n",
    "y_true = [0,1,1,1,1,1,1,1,0]\n",
    "y_pred = [0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "print(\"MCC =\",matthews_correlation_coefficient(y_true,y_pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"2 falsos negativos e 7 positivos verdadeiros\\n\")\n",
    "y_true = [1,1,1,1,1,1,1,1,1]\n",
    "y_pred = [0,1,1,1,1,1,1,1,0]\n",
    "\n",
    "print(\"MCC =\",matthews_correlation_coefficient(y_true,y_pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"7 positivos verdadeiros e 2 negativos verdadeiros\\n\")\n",
    "y_true = [1,1,1,0,0,1,1,1,1]\n",
    "y_pred = [1,1,1,0,0,1,1,1,1]\n",
    "\n",
    "print(\"MCC =\",matthews_correlation_coefficient(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **REGRESSÃO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Error (Erro Médio Absoluto)\n",
    "\n",
    "O Mean Absolute Error (MAE) é uma métrica de avaliação usada principalmente em problemas de regressão para medir o quão próximo as previsões do modelo estão dos valores reais. É uma métrica que calcula a média das diferenças absolutas entre as previsões do modelo e os valores reais.\n",
    "\n",
    "A fórmula para calcular o MAE é a seguinte:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{true}}^{(i)} - y_{\\text{pred}}^{(i)}|\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $n$ é o número total de exemplos no conjunto de dados.\n",
    "\n",
    "- $y_{\\text{true}}^{(i)}$ representa o valor real do i-ésimo exemplo.\n",
    "\n",
    "- $y_{\\text{pred}}^{(i)}$ representa a previsão do modelo para o i-ésimo exemplo.\n",
    "\n",
    "O módulo evita o cancelamento de erros negativos e positivos, entretanto, o módulo atrapalha o cálculo do gradiente e, por isso, não é utilizado em funções de custo.\n",
    "\n",
    "O MAE mede o erro médio absoluto das previsões do modelo em relação aos valores reais. Quanto menor o valor do MAE, melhor o desempenho do modelo, pois indica que as previsões estão mais próximas dos valores reais. Um MAE igual a 0 indica que o modelo faz previsões perfeitas.\n",
    "\n",
    "O MAE é uma métrica útil para entender o desempenho de modelos de regressão e comparar diferentes modelos em termos de quão bem eles se ajustam aos dados observados.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    # Verifica se as listas têm o mesmo comprimento\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"As listas y_true e y_pred devem ter o mesmo comprimento.\")\n",
    "\n",
    "    # Calcula o MAE\n",
    "    mae = sum(abs(y_true[i] - y_pred[i]) for i in range(len(y_true))) / len(y_true)\n",
    "\n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true = [10,4,7,4,7,12,4,2,0]\n",
      "y_pred = [8,4,7,3,4,10,3,2,1]\n",
      "MNE = 1.1111111111111112 \n",
      "\n",
      "y_true = [10,4,7,4,7,12,4,2,0]\n",
      "y_pred = [10,4,7,4,7,12,4,2,0]\n",
      "MNE = 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [10,4,7,4,7,12,4,2,0]\n",
    "y_pred = [8,4,7,3,4,10,3,2,1]\n",
    "\n",
    "print(\"y_true = [10,4,7,4,7,12,4,2,0]\\ny_pred = [8,4,7,3,4,10,3,2,1]\\nMNE =\", mean_absolute_error(y_true,y_pred),\"\\n\")\n",
    "\n",
    "y_true = [10,4,7,4,7,12,4,2,0]\n",
    "y_pred = [10,4,7,4,7,12,4,2,0]\n",
    "\n",
    "print(\"y_true = [10,4,7,4,7,12,4,2,0]\\ny_pred = [10,4,7,4,7,12,4,2,0]\\nMNE =\", mean_absolute_error(y_true,y_pred),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE)\n",
    "\n",
    "O Mean Squared Error (MSE) é uma métrica de avaliação frequentemente usada em problemas de regressão para medir o erro médio quadrático das previsões em relação aos valores reais. É uma métrica importante para avaliar o quão bem um modelo de regressão está se ajustando aos dados.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $n$ é o número de exemplos no conjunto de dados.\n",
    "- $y_i$ são os valores reais.\n",
    "- $\\hat{y}_i$ são as previsões do modelo.\n",
    "\n",
    "O MSE calcula a média dos erros quadráticos, dando maior peso a erros maiores. Portanto, erros maiores contribuem mais para o valor total do MSE.\n",
    "\n",
    "Quanto menor o valor do MSE, melhor, pois indica que as previsões do modelo estão mais próximas dos valores reais. Por outro lado, um valor mais alto do MSE indica que as previsões estão mais distantes dos valores reais, o que significa um desempenho pior do modelo de regressão.\n",
    "\n",
    "O MSE é uma métrica comum usada para avaliar a qualidade de modelos de regressão, e muitas vezes é usado em conjunto com outras métricas para fornecer uma imagem completa do desempenho do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "\n",
    "    # Verifica se as listas têm o mesmo comprimento\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"As listas y_true e y_pred devem ter o mesmo comprimento.\")\n",
    "\n",
    "    # Transformando as listas em arrays NumPy para facilitar os cálculos\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calcula o MSE\n",
    "    mse = sum((y_true[i] - y_pred[i])**2 for i in range(len(y_true))) / len(y_true)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true = [10,4,7,4,7,12,4,2,0]\n",
      "y_pred = [8,4,7,3,4,10,3,2,1]\n",
      "MSE = 2.2222222222222223 \n",
      "\n",
      "y_true = [10,4,7,4,7,12,4,2,0]\n",
      "y_pred = [10,4,7,4,7,12,4,2,0]\n",
      "MSE = 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [10,4,7,4,7,12,4,2,0]\n",
    "y_pred = [8,4,7,3,4,10,3,2,1]\n",
    "\n",
    "print(\"y_true = [10,4,7,4,7,12,4,2,0]\\ny_pred = [8,4,7,3,4,10,3,2,1]\\nMSE =\", mean_squared_error(y_true,y_pred),\"\\n\")\n",
    "\n",
    "y_true = [10,4,7,4,7,12,4,2,0]\n",
    "y_pred = [10,4,7,4,7,12,4,2,0]\n",
    "\n",
    "print(\"y_true = [10,4,7,4,7,12,4,2,0]\\ny_pred = [10,4,7,4,7,12,4,2,0]\\nMSE =\", mean_squared_error(y_true,y_pred),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error (RMSE)\n",
    "\n",
    "O Root Mean Squared Error (RMSE) é uma métrica de avaliação de desempenho usada em problemas de regressão para medir o erro médio quadrático das previsões em relação aos valores reais. É uma métrica valiosa para avaliar a qualidade de modelos de regressão, pois fornece uma medida interpretável do desvio padrão dos erros das previsões.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $n$ é o número de exemplos no conjunto de dados.\n",
    "- $y_i$ são os valores reais.\n",
    "- $\\hat{y}_i$ são as previsões do modelo.\n",
    "\n",
    "O RMSE calcula a raiz quadrada da média dos erros quadráticos, dando uma medida de desvio padrão dos erros das previsões. Quanto menor o valor do RMSE, melhor, pois indica que as previsões do modelo estão mais próximas dos valores reais.\n",
    "\n",
    "O RMSE é uma métrica útil para avaliar a qualidade de modelos de regressão e é frequentemente usado em conjunto com o MSE para fornecer uma medida mais interpretável do erro. No RMSE, os erros maiores têm um peso maior devido à raiz quadrada, tornando-o mais sensível a erros significativos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true = [10,4,7,4,7,12,4,2,0]\n",
      "y_pred = [8,4,7,3,4,10,3,2,1]\n",
      "RMSE = 1.4907119849998598 \n",
      "\n",
      "y_true = [10,4,7,4,7,12,4,2,0]\n",
      "y_pred = [10,4,7,4,7,12,4,2,0]\n",
      "RMSE = 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [10,4,7,4,7,12,4,2,0]\n",
    "y_pred = [8,4,7,3,4,10,3,2,1]\n",
    "\n",
    "print(\"y_true = [10,4,7,4,7,12,4,2,0]\\ny_pred = [8,4,7,3,4,10,3,2,1]\\nRMSE =\", root_mean_squared_error(y_true,y_pred),\"\\n\")\n",
    "\n",
    "y_true = [10,4,7,4,7,12,4,2,0]\n",
    "y_pred = [10,4,7,4,7,12,4,2,0]\n",
    "\n",
    "print(\"y_true = [10,4,7,4,7,12,4,2,0]\\ny_pred = [10,4,7,4,7,12,4,2,0]\\nRMSE =\", root_mean_squared_error(y_true,y_pred),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "O **Mean Absolute Percentage Error (MAPE)** é uma métrica usada em problemas de regressão para avaliar o desempenho de modelos de previsão. Ele mede a porcentagem média de erro absoluto em relação aos valores reais. O MAPE é particularmente útil quando se deseja entender o erro relativo das previsões em relação aos valores reais, expressando-o como uma porcentagem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $n$ é o número de exemplos no conjunto de dados.\n",
    "- $y_i$ são os valores reais.\n",
    "- $\\hat{y}_i$ são as previsões do modelo.\n",
    "\n",
    "O MAPE calcula a diferença percentual média entre as previsões do modelo $\\hat{y}_i$ e os valores reais $y_i$. Essa métrica é expressa como uma porcentagem, o que a torna facilmente interpretável.\n",
    "\n",
    "O MAPE tem algumas limitações, incluindo a sensibilidade a valores zero nos dados reais, que podem levar a divisões por zero.\n",
    "\n",
    "No MAPE, um valor menor indica um melhor ajuste do modelo às previsões, uma vez que representa uma menor porcentagem de erro em relação aos valores reais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    mape = (100 / n) * sum(abs((y_true[i] - y_pred[i]) / y_true[i]) for i in range(n) if y_true[i] != 0)\n",
    "\n",
    "    return mape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true = [10,4,7,4,7,12,4,2,1]\n",
      "y_pred = [8,4,7,3,4,10,3,2,1]\n",
      "MAPE = 14.391534391534393 \n",
      "\n",
      "y_true = [10,4,7,4,7,12,4,2,3]\n",
      "y_pred = [10,4,7,4,7,12,4,2,2]\n",
      "MAPE = 3.7037037037037033 \n",
      "\n",
      "y_true = [10,4,7,4,7,12,4,2,3]\n",
      "y_pred = [10,4,7,4,7,12,4,2,3]\n",
      "MAPE = 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [10,4,7,4,7,12,4,2,1]\n",
    "y_pred = [8,4,7,3,4,10,3,2,1]\n",
    "\n",
    "print(\"y_true = [10,4,7,4,7,12,4,2,1]\\ny_pred = [8,4,7,3,4,10,3,2,1]\\nMAPE =\", mean_absolute_percentage_error(y_true,y_pred),\"\\n\")\n",
    "\n",
    "y_true = [10,4,7,4,7,12,4,2,3]\n",
    "y_pred = [10,4,7,4,7,12,4,2,2]\n",
    "\n",
    "print(\"y_true = [10,4,7,4,7,12,4,2,3]\\ny_pred = [10,4,7,4,7,12,4,2,2]\\nMAPE =\", mean_absolute_percentage_error(y_true,y_pred),\"\\n\")\n",
    "\n",
    "y_true = [10,4,7,4,7,12,4,2,3]\n",
    "y_pred = [10,4,7,4,7,12,4,2,3]\n",
    "\n",
    "print(\"y_true = [10,4,7,4,7,12,4,2,3]\\ny_pred = [10,4,7,4,7,12,4,2,3]\\nMAPE =\", mean_absolute_percentage_error(y_true,y_pred),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
